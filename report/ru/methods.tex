\subsection{Данные}
Для каждой решетки размера $L \in [64, 128, 256, 512]$ мы сгенерировали выборку $\{s_i\}_{i=1}^{N}$в модели Манна и БТВ размера $N=10^8$, пропустив первые $10 \cdot L^2$ событий, чтобы насытить решетку песчинками. Затем мы разбили данные пополам на тренировочную и тестовую части.

\subsection{Модель}
Чтобы качество прогноза не зависело от стороны решетки, мы вводим шкалирование размеров прогнозируемых крупных событий $X_i = I[s_i > \eta(L)]$. Для скейлинг-фукнции $\eta(L)$ мы предлагаем формулу

$$ \eta(L) = p \cdot L^\gamma, $$

\noindent где $\gamma$ -- подбираемый параметр, который различается для моделей Манна и БТВ, а $p$ --- некоторая константа, большая $0$, не зависящая от стороны решетки и регулирующая частоту прогнозирующих событий.

\subsection{Алгоритм}

Наши прогнозы будут основаны на переменной принятия решения $y_i$, введенной в~\cite{Hallerberg2009,Kantz2010}, формула которой похожа на измененный AR(1)-процесс, изучавшийся в~\cite{Lewis1985}:

$$y_i = \sum\limits_{k=1}^{i} a^k \cdot s_{i-k}, $$

\noindent где $a$ --- подбираемый параметр, переведенный в логарифмическую шкалу: $a = \exp\left(-\frac{1}{T}\right)$~\cite{Hallerberg2009}.

На тренировочной выборке для каждой решетки по-отдельности мы оцениваем условную вероятность $P(X=1\ |\ y)$. Для этого мы разбиваем значения $y$ на $200$ равных бинов, и в каждом вычисляем условную вероятность по формуле:

$$ \hat{P}(X_i = 1\ | y_i \in y_{bin}) = \frac{\Count\limits_{j}(X_j = 1\ |\ y_{j} \in y_{bin})}{\Count\limits_{j}(X_j\ |\ y_{j} \in y_{bin})} $$

Для прогноза на тестовой выборке мы так же вычисляем перемененную принятия решения, по которой выдаем вероятность крупного события, оцененную по формуле.

\subsection{Метрики}

Для анализа качества модели мы будем использовать ROC-кривые~\cite{Molchan1997}, поскольку перед нами стоит задача бинарной классификации с несбалансированной выборкой (см. Приложение \ref{appendix:c}).  Количественной интерпретацией ROC-кривой, которую мы будем использовать для оценки алгоритма, является метрика $\epsilon(L,\gamma,p,T)$, равная Манхетонскому расстоянию от точки $(0, 1)$ до ближайшей к ней точке на ROC-кривой~\cite{Hallerberg2009,Shapoval2006}, построенной по прогнозу модели с фиксированными параметрами $L$, $\gamma$, $p$ и $T$.

Поскольку в нашей работе мы ищем шкалиремую предсказательную модель, необходимо численно оценивать качество шкалирования для разных $\gamma$. С точки зрения метрики $\epsilon$, идеальный параметр шкалирования $\gamma$ --- это тот, в результате которого для всех фиксированных $p$ качество модели $\epsilon_{\gamma,p,T}(L)$ с оптимально подобранным $T$ не зависит от $L$. Это значит, что абсолютное значение углового коэффициента $k(p, \gamma)$ регрессирующей прямой, проходящей через точки на кривой $\epsilon_{\gamma,p,T}(L)$ должен быть близок к $0$, а сами точки иметь близкую к нулю дисперсию $v(p, \gamma)$.

С другой стороны, $\epsilon$ --- это лишь одна точка на ROC-кривой, которая не позволяет сказать о качестве шкалируемости целиком. По этой причине, чтобы показать, что алгоритм обладает свойством шкалируемости, в работах~\cite{Hallerberg2009,deluca} добиваются наложения ROC-кривых. Поэтому мы предлагаем численно оценивать качество наложения ROC-кривых с помощью Intersection over Union (IoU) метрики~\cite{Murphy1996,Cordts2016,Alhaija2018}, примененной к ROC-кривым.
